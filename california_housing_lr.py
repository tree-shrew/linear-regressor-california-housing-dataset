# -*- coding: utf-8 -*-
"""California_housing_LR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fbM_MNHHoOiJ8lBQ9P1boZnIMLJabZxH

# **Regression Problem Statement:** Apply Linear Regression model to predict median household values for California districts based off of attributes derived from the 1990 US Census Data.

## *Overview of the data*
"""

# importing dependencies

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.datasets import fetch_california_housing

from sklearn.preprocessing import MinMaxScaler, StandardScaler

from sklearn.feature_selection import (SelectPercentile, SelectKBest
                                       , mutual_info_regression, f_regression
                                       , RFE, SelectFromModel, SequentialFeatureSelector
                                       )
from sklearn.decomposition import PCA

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, Lasso, Ridge
from sklearn.metrics import r2_score, mean_squared_error

# loading data

data = fetch_california_housing(as_frame=True)
data

data['frame'].head(3)

data.frame.info()

data.frame.describe().T

"""## *Pre-processing*"""

'''
The dataset is clean with all numeric and 0 null values.
Hence proceeding with normalization & feature selection/dimensionality reduction transformations.
'''

"""### **Normalization:** using MinMaxScaler"""

scaled_data = MinMaxScaler().fit_transform(data.data)
df = pd.DataFrame(scaled_data, columns=data.feature_names)
df.head()

"""### **Feature Selection**

#### *1. Filter based selection*

##### **Mutual Information**
"""

# Scoring function as 'Mutual Information' with SelectKBest as the transformer

x, y = df, data.target

# Since no. of features is less, looping through all possible k values to find the optimal no. of features

scores = {'Features':[],'R2':[],'MSE':[]}

for k in range(1, x.shape[1]+1):
    selector = SelectKBest(mutual_info_regression, k=k)
    x_new = selector.fit_transform(x, y)
    x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size=0.2, random_state=8)
    model = LinearRegression()
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    scores['Features'].append(k)
    scores['R2'].append(r2_score(y_test, y_pred))
    scores['MSE'].append(mean_squared_error(y_test, y_pred))

scores_df = pd.DataFrame(scores)
scores_df

'''
k=6 results in the best R2 score (59.6%)
However nearly similar R2 scores are achieved with k=8/6/4 (61.7%, 60.3%, 59.6% respectively)
Hence choosing k=3 as the optimal value considering it provides comparative R2 with way less computational cost

'''

# k = 3

selector = SelectKBest(mutual_info_regression, k=3).fit(x, y)
x_new = selector.transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size=0.2, random_state=8)

model = LinearRegression()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

r2_mi = r2_score(y_test, y_pred)
mse_mi = mean_squared_error(y_test, y_pred)
print(r2_mi, mse_mi)
print(selector.get_feature_names_out())

plt.figure(figsize=(9, 4))
plt.bar(data.feature_names, selector.scores_)
plt.show()

"""##### **Pearson's Correlation:** Univariate analysis"""

# Scoring function as f_test with SelectKBest as the transformer

x, y = df, data.target

# Since no. of features is less, looping through all possible k values to find the optimal no. of features

scores = {'Features':[],'R2':[],'MSE':[]}

for k in range(1, x.shape[1]+1):
    selector = SelectKBest(f_regression, k=k)
    x_new = selector.fit_transform(x, y)
    x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size=0.2, random_state=8)
    model = LinearRegression()
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    scores['Features'].append(k)
    scores['R2'].append(r2_score(y_test, y_pred))
    scores['MSE'].append(mean_squared_error(y_test, y_pred))

scores_df = pd.DataFrame(scores)
scores_df

'''
Selecting k=6 for providing near best R2 (61.7%) with minimal features

'''

# k = 6

selector = SelectKBest(f_regression, k=6).fit(x, y)
x_new = selector.transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size=0.2, random_state=8)

model = LinearRegression()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

r2_ft = r2_score(y_test, y_pred)
mse_ft = mean_squared_error(y_test, y_pred)
print(r2_ft, mse_ft)
print(selector.get_feature_names_out())

"""##### **Pearson's Correlation:** Unsupervised analysis"""

# drawing correlation heatmap

sns.heatmap(df.corr(), annot=True, linewidth=0.5, cmap='Blues', fmt='.1f')

'''
High collinearity is seen b/w 'AveRooms' and 'AveBedrms'
Hence, removing either feature based on lesser correlation with target

'''

sns.heatmap(data.frame.corr(), annot=True, linewidth=0.5, cmap='Blues', fmt='.1f')

# Dropping 'AveBedrms' to deal with multicollonearity, since it shows minimal correlation with the target
# Further dropping 'AveOccup' & 'Population' due to minimal correlation with the target

x, y = df, data.target
x_new = df.drop(columns=['AveBedrms', 'AveOccup', 'Population'])

x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size=0.2, random_state=8)

model = LinearRegression()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

r2_hmap = r2_score(y_test, y_pred)
mse_hmap = mean_squared_error(y_test, y_pred)
print(r2_hmap, mse_hmap)

"""#### *2. Wrapper based selection*

##### **Recursive Feature Elimination**
"""

# Using unscaled data & Lasso estimator

x, y = data.data, data.target

# looping for all possible n_features_to_select

scores = {'Features':[],'R2':[],'MSE':[]}

for n in range(1, x.shape[1]+1):
    estimator = Lasso()
    selector = RFE(estimator, n_features_to_select=n, step=1)
    x_new = selector.fit_transform(x, y)
    x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size=0.2, random_state=8)
    model = LinearRegression()
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    scores['Features'].append(n)
    scores['R2'].append(r2_score(y_test, y_pred))
    scores['MSE'].append(mean_squared_error(y_test, y_pred))

scores_df = pd.DataFrame(scores)
scores_df

'''
Selecting the optimal no. of features as n_features_to_select = 5 for providing near highest R2 (60.2%)

'''

# n_features_to_select = 5

estimator = Lasso()
selector = RFE(estimator, n_features_to_select=5, step=1).fit(x, y)
x_new = selector.transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size=0.2, random_state=8)

model = LinearRegression()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

r2_rfe = r2_score(y_test, y_pred)
mse_rfe = mean_squared_error(y_test, y_pred)
print(r2_rfe, mse_rfe)
print(selector.get_feature_names_out())

"""##### **Select From Model**"""

# Using Ridge as the estimator

x, y = df, data.target

estimator = Ridge().fit(x, y)
selector = SelectFromModel(estimator, threshold='mean', prefit=True).fit(x, y)

x_new = selector.transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size=0.2, random_state=8)

model = LinearRegression()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

r2_sfm = r2_score(y_test, y_pred)
mse_sfm = mean_squared_error(y_test, y_pred)
print(r2_sfm, mse_sfm)
print(selector.get_feature_names_out())

"""##### **Sequential Feature Selection**"""

x, y = df, data.target

estimator = Ridge()

selector = SequentialFeatureSelector(estimator, n_features_to_select='auto', direction='backward').fit(x, y)
x_new = selector.transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size=0.2, random_state=8)

model = LinearRegression()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

r2_sfs = r2_score(y_test, y_pred)
mse_sfs = mean_squared_error(y_test, y_pred)

print(r2_sfs, mse_sfs)
print(selector.get_feature_names_out())

"""#### *3. Intrinsic/Embedded selection*

##### **L1 Regularization**
"""

x, y = df, data.target

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=8)

model = LassoCV()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

r2_l1 = r2_score(y_test, y_pred)
mse_l1 = mean_squared_error(y_test, y_pred)

print(r2_l1, mse_l1)

print(model.coef_)
print(model.intercept_)
print(model.score(x, y))

"""### **Dimensionality Reduction**

#### *Principal Component Analysis*
"""

x, y = df, data.target

scores = {'Features':[],'R2':[],'MSE':[]}

for n in range(1, x.shape[1]+1):
    pca = PCA(n_components=n, svd_solver='full')
    x_new = pca.fit_transform(x)
    x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size=0.2, random_state=8)
    model = LinearRegression()
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    scores['Features'].append(n)
    scores['R2'].append(r2_score(y_test, y_pred))
    scores['MSE'].append(mean_squared_error(y_test, y_pred))

scores_df = pd.DataFrame(scores)
scores_df

'''
Selecting n_components = 4 for providing near best R2 (60.1%)

'''

# n_components = 4

x_new = PCA(n_components=4, svd_solver='full').fit_transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size=0.2, random_state=8)

model = LinearRegression()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

r2_pca = r2_score(y_test, y_pred)
mse_pca = mean_squared_error(y_test, y_pred)

print(r2_pca, mse_pca)

"""### **Revisiting each method by setting threshold of 4 features for later evaluation**"""

# Mutual Information for Regression

# k = 4

selector = SelectKBest(mutual_info_regression, k=4).fit(x, y)
x_new = selector.transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size=0.2, random_state=8)

model = LinearRegression()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

r2_mi4 = r2_score(y_test, y_pred)
mse_mi4 = mean_squared_error(y_test, y_pred)
print(r2_mi4, mse_mi4)
print(selector.get_feature_names_out())

# f_Regression

# k = 4

selector = SelectKBest(f_regression, k=4).fit(x, y)
x_new = selector.transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size=0.2, random_state=8)

model = LinearRegression()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

r2_ft4 = r2_score(y_test, y_pred)
mse_ft4 = mean_squared_error(y_test, y_pred)
print(r2_ft4, mse_ft4)
print(selector.get_feature_names_out())

# Feature vs Feature Correlation: Heatmap

# Dropping 'AveBedrms' to deal with multicollonearity, since it shows minimal correlation with the target
# Further dropping 'HouseAge', 'AveOccup' & 'Population' due to minimal correlation with the target

x, y = df, data.target
x_new = df.drop(columns=['HouseAge', 'AveRooms', 'AveBedrms', 'AveOccup', 'Population'])

x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size=0.2, random_state=8)

model = LinearRegression()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

r2_hmap4 = r2_score(y_test, y_pred)
mse_hmap4 = mean_squared_error(y_test, y_pred)
print(r2_hmap4, mse_hmap4)

# RFE

# n_features_to_select = 4

x, y = data.data, data.target

estimator = Lasso()
selector = RFE(estimator, n_features_to_select=4, step=1).fit(x, y)
x_new = selector.transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size=0.2, random_state=8)

model = LinearRegression()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

r2_rfe4 = r2_score(y_test, y_pred)
mse_rfe4 = mean_squared_error(y_test, y_pred)
print(r2_rfe4, mse_rfe4)
print(selector.get_feature_names_out())

# SFM

r2_sfm4 = r2_sfm
mse_sfm4 = mse_sfm
print(r2_sfm4, mse_sfm4)

# SFS

r2_sfs4 = r2_sfs
mse_sfs4 = mse_sfs
print(r2_sfs4, mse_sfs4)

# PCA

r2_pca4 = r2_pca
mse_pca4 = mse_pca
print(r2_pca4, mse_pca4)

"""### **Fitting without feature selection**"""

x, y = df, data.target

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=8)

model = LinearRegression()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

print(r2, mse)

"""## *Evaluation*"""

# Plotting best r2 scores and mse for each pre-processing method

r2_scores = [r2_mi, r2_ft, r2_hmap, r2_rfe, r2_sfm, r2_sfs, r2_l1, r2_pca]
mse_scores = [mse_mi, mse_ft, mse_hmap, mse_rfe, mse_sfm, mse_sfs, mse_l1, mse_pca]
labels = ['MI', 'f_test', 'Heatmap', 'RFE', 'SFM', 'SFS', 'L1', 'PCA']

plt.figure(figsize=(6, 5))
plt.plot(labels, r2_scores, marker='o', label='R2')
plt.plot(labels, mse_scores, marker='o', label='MSE')
plt.legend()
plt.title("Best R2 and MSE for each pre-processing method")
plt.xlabel("Pre-processing method")
plt.ylabel("Score")
plt.show()

# Plotting r2 scores and mse for each method, where no. of features were limited to 4

r2_scores = [r2_mi4, r2_ft4, r2_hmap4, r2_rfe4, r2_sfm, r2_sfs, r2_pca]
mse_scores = [mse_mi4, mse_ft4, mse_hmap4, mse_rfe4, mse_sfm, mse_sfs, mse_pca]
labels = ['MI', 'f_test', 'Heatmap', 'RFE', 'SFM', 'SFS', 'PCA']

plt.figure(figsize=(6, 5))
plt.plot(labels, r2_scores, marker='o', label='R2')
plt.plot(labels, mse_scores, marker='o', label='MSE')
plt.legend()
plt.title("Best R2 and MSE for each method applied on 4 features")
plt.xlabel("Pre-processing method")
plt.ylabel("Score")
plt.show()

# Plotting 'No feature selection' vs 'Best of 6 features' vs 'Best of 4 features'

r2_scores = [r2, r2_ft, r2_sfs4]
mse_scores = [mse, mse_ft, mse_sfs4]
labels = ['8 features', '6 features (f_test)', '4 features (SFS)']

plt.figure(figsize=(6, 5))
plt.plot(labels, r2_scores, marker='o', label='R2')
plt.plot(labels, mse_scores, marker='o', label='MSE')
for a, b in zip(labels, r2_scores):
    plt.text(a, b, str(round(b, 4)), va='top', ha='center')
for a, b in zip(labels, mse_scores):
    plt.text(a, b, str(round(b, 4)), va='bottom', ha='center')
plt.legend()
plt.show()

"""## *Result*

### **Feature selection through f_test (Pearson's correlation) on scaled data (MinMaxScaler) provided the best accuracy (R2 = 61.69%, MSE = 0.5029) with minimal computational cost (k = 6)**

### **If the feature threshold is set to 4, feature selection through SFS provides near to best accuracy (R2 = 60.22%, MSE = 0.5222)**

### **Summarized Results**
"""

results = {'method': ["Without feature selection (features=8)", "f_regression transformer (k=6)", "SFS transformer (n=4)"]
           , 'R2 Score': [r2, r2_ft, r2_sfs4]
           , 'MSE': [mse, mse_ft, mse_sfs4]}

df_results = pd.DataFrame(results)
df_results

